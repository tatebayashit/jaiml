## ğŸ”§ CI_SRS v1.0 ã‚·ã‚¹ãƒ†ãƒ è¦æ±‚ä»•æ§˜æ›¸ - æ”¹è¨‚ç‰ˆ

### A. çµ±ä¸€è¨˜è¿°ã‚»ã‚¯ã‚·ãƒ§ãƒ³

#### A.1 æ¦‚è¦

**ã‚·ã‚¹ãƒ†ãƒ å**: JAIMLç¶™ç¶šçš„ã‚¤ãƒ³ãƒ†ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆCIï¼‰ã‚·ã‚¹ãƒ†ãƒ 

**ç›®çš„**: JAIMLãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®3ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆJAIMLæœ¬ä½“ã€lexicon_expansionã€vector_pretrainerï¼‰é–“ã®ä»•æ§˜æ•´åˆæ€§ã€å†ç¾æ€§ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚’è‡ªå‹•æ¤œè¨¼ã—ã€å“è³ªã‚’ä¿è¨¼ã™ã‚‹ã€‚

#### A.2 ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹æˆã¨è²¬å‹™

```
src/ci/
â”œâ”€â”€ schema_validate.py           # YAMLè¨­å®šæ¤œè¨¼
â”œâ”€â”€ check_tokenizer.py           # tokenizerçµ±ä¸€æ€§æ¤œæŸ»
â”œâ”€â”€ check_versions.py            # ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•´åˆæ€§æ¤œæŸ»
â”œâ”€â”€ check_security.py            # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ¤œæŸ»
â”œâ”€â”€ check_jsonl.py               # JSONLå½¢å¼æ¤œè¨¼
â”œâ”€â”€ check_anonymization.py       # åŒ¿ååŒ–å‡¦ç†æ¤œè¨¼
â”œâ”€â”€ check_cluster_quality.py     # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å“è³ªæ¤œè¨¼
â”œâ”€â”€ check_annotation_metrics.py  # ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹æ¤œè¨¼
â””â”€â”€ run_all_checks.py            # çµ±åˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ

.github/workflows/
â”œâ”€â”€ validate.yml                 # è¨­å®šæ¤œè¨¼ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
â”œâ”€â”€ tests.yml                    # ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
â””â”€â”€ security.yml                 # ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ¤œæŸ»ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
```

**è²¬å‹™**:
- è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®æ§‹æ–‡ãƒ»å€¤ã®æ•´åˆæ€§æ¤œè¨¼
- ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«é–“ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®ä¸€è²«æ€§ç¢ºèª
- ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒªã‚¹ã‚¯ã®è‡ªå‹•æ¤œå‡º
- ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸ã®ç›£è¦–

#### A.3 å…¥å‡ºåŠ›ä»•æ§˜

**å…¥åŠ›**:
- å„ç¨®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆYAMLå½¢å¼ï¼‰
- ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ï¼ˆPythonï¼‰
- ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ

**å‡ºåŠ›**:
- æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆï¼ˆJSON/Markdownå½¢å¼ï¼‰
- ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°
- ã‚«ãƒãƒ¬ãƒƒã‚¸ãƒ¬ãƒãƒ¼ãƒˆ

#### A.4 ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å®šç¾©

**CIè¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- `python_version`: "3.11"
- `test_coverage_threshold`: 80  # å…¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å…±é€šã®ã‚«ãƒãƒ¬ãƒƒã‚¸é–¾å€¤ï¼ˆ%ï¼‰
- `max_complexity`: 10ï¼ˆå¾ªç’°çš„è¤‡é›‘åº¦ï¼‰
- `allowed_licenses`: ["MIT", "Apache-2.0", "BSD"]
- `min_silhouette_score`: 0.25  # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å“è³ªé–¾å€¤

#### A.5 é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
config/
â”œâ”€â”€ global.yaml                  # æ¤œè¨¼å¯¾è±¡
â””â”€â”€ tfidf_config.yaml           # æ¤œè¨¼å¯¾è±¡

ci/
â”œâ”€â”€ schemas/
â”‚   â”œâ”€â”€ global_schema.yaml      # global.yamlã®ã‚¹ã‚­ãƒ¼ãƒ
â”‚   â””â”€â”€ tfidf_schema.yaml       # tfidf_config.yamlã®ã‚¹ã‚­ãƒ¼ãƒ
â””â”€â”€ reports/                    # æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›å…ˆ
```

#### A.6 ä½¿ç”¨ä¾‹ã¨ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³

**å€‹åˆ¥æ¤œè¨¼**:
```bash
# YAMLè¨­å®šæ¤œè¨¼
python ci/schema_validate.py

# tokenizerçµ±ä¸€æ€§æ¤œæŸ»
python ci/check_tokenizer.py

# ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ¤œæŸ»
python ci/check_security.py

# åŒ¿ååŒ–æ¤œè¨¼
python ci/check_anonymization.py --corpus-dir corpus/jsonl/

# ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å“è³ªæ¤œè¨¼
python ci/check_cluster_quality.py --metrics-file outputs/reports/cluster_metrics.json
```

**çµ±åˆæ¤œè¨¼**:
```bash
python ci/run_all_checks.py --output-format json
```

#### A.7 CIæ¤œè¨¼é …ç›®

1. **è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«æ•´åˆæ€§**: å…¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®è¨­å®šå€¤ä¸€è‡´
2. **ä¾å­˜é–¢ä¿‚æ•´åˆæ€§**: requirements.txtã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¯„å›²
3. **ã‚³ãƒ¼ãƒ‰å“è³ª**: flake8ã€mypyã€blackæº–æ‹ 
4. **ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸**: 80%ä»¥ä¸Š
5. **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£**: banditã€safetyæ¤œæŸ»
6. **Annotation Metrics Check**: ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å“è³ªã®å®šæœŸç›£è¦–
   - Îº ã¨ Macro-F1 ã‚’ nightly job ã§è¨ˆç®—
   - Îº<0.60 or Macro-F1<0.60 ã§ Warning
7. **äººæ‰‹ç¢ºèªãƒ•ãƒ©ã‚°**: åŒ¿ååŒ–ãƒ‡ãƒ¼ã‚¿ã®äººæ‰‹ç¢ºèªçŠ¶æ…‹
8. **ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å“è³ª**: Silhouette score â‰¥ 0.25
9. **è¾æ›¸ãƒãƒƒã‚·ãƒ¥æ•´åˆæ€§**: changelogè¨˜è¼‰ã®ãƒãƒƒã‚·ãƒ¥ã¨å®Ÿãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€è‡´
10. **TF-IDFå†ç¾æ€§**: åŒä¸€å…¥åŠ›ã‹ã‚‰åŒä¸€noveltyã‚¹ã‚³ã‚¢
11. **åŒ¿ååŒ–å‡¦ç†**: å€‹äººæƒ…å ±ã®é©åˆ‡ãªãƒã‚¹ã‚­ãƒ³ã‚°
12. **ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°æ•´åˆæ€§**: JSONã‚¹ã‚­ãƒ¼ãƒæº–æ‹ ã¨ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è«–ç†æ€§
13. **UIä»•æ§˜æ•´åˆæ€§**: ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®šç¾©ã¨ã®è¡¨ç¤ºè¦ç´ ã®ä¸€è‡´
14. **ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å“è³ª**: Weighted-Îº â‰¥ 0.60, Macro-F1 â‰¥ 0.60

#### A.8 ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å®šç¾©ï¼ˆå‹æ³¨é‡ˆä»˜ãï¼‰

```python
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass

@dataclass
class ValidationResult:
    """æ¤œè¨¼çµæœã‚’è¡¨ã™ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    passed: bool
    errors: List[str]
    warnings: List[str]
    info: Dict[str, any]

class ConfigValidator:
    def validate_yaml(self, config_path: str, schema_path: str) -> ValidationResult:
        """YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®æ¤œè¨¼"""

class ConsistencyChecker:
    def check_tokenizer_consistency(self) -> ValidationResult:
        """å…¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®tokenizerè¨­å®šä¸€è²«æ€§ã‚’æ¤œè¨¼"""
    
    def check_parameter_consistency(self, param_name: str) -> ValidationResult:
        """ç‰¹å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ä¸€è²«æ€§ã‚’æ¤œè¨¼"""

class SecurityAuditor:
    def scan_for_pickle_usage(self) -> ValidationResult:
        """Pickleä½¿ç”¨ç®‡æ‰€ã®æ¤œå‡º"""
    
    def check_dependencies(self) -> ValidationResult:
        """ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®è„†å¼±æ€§æ¤œæŸ»"""
def check_dependencies(self) -> ValidationResult:
         """ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®è„†å¼±æ€§æ¤œæŸ»"""

class AnonymizationChecker:
    def check_anonymization_flags(self, corpus_dir: str) -> ValidationResult:
        """åŒ¿ååŒ–ãƒ•ãƒ©ã‚°ã®æ¤œè¨¼"""
    
    def scan_unmasked_patterns(self, corpus_dir: str) -> ValidationResult:
        """æœªãƒã‚¹ã‚¯å€‹äººæƒ…å ±ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º"""

class LexiconValidator:
    def check_canonical_keys(self, lexicon_path: str) -> ValidationResult:
        """canonical_keyã®æ­£è¦åŒ–ãƒ«ãƒ¼ãƒ«æº–æ‹ ã‚’æ¤œè¨¼"""

class AnnotationQualityChecker:
    def check_annotation_metrics(self, annotation_file: str) -> ValidationResult:
        """ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ¤œè¨¼"""
    
    def validate_annotation_logs(self, log_dir: str) -> ValidationResult:
        """ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°ã®ã‚¹ã‚­ãƒ¼ãƒæº–æ‹ æ€§æ¤œè¨¼"""
    
    def check_ui_consistency(self, snapshot_file: str) -> ValidationResult:
        """UIä»•æ§˜ã¨ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®šç¾©ã®æ•´åˆæ€§æ¤œè¨¼"""      
```

#### A.9 æ—¢çŸ¥ã®åˆ¶ç´„ã¨æ³¨æ„äº‹é …

1. **å®Ÿè¡Œç’°å¢ƒ**: Ubuntu 20.04/22.04ã®ã¿å®Œå…¨ã‚µãƒãƒ¼ãƒˆ
2. **Pythonç‰ˆ**: 3.8ã€œ3.11ã®ã¿å¯¾å¿œ
3. **å®Ÿè¡Œæ™‚é–“**: å…¨æ¤œè¨¼å®Œäº†ã¾ã§æœ€å¤§10åˆ†
4. **ä¸¦åˆ—å®Ÿè¡Œ**: ä¸€éƒ¨ã®æ¤œè¨¼ã¯æ’ä»–åˆ¶å¾¡ãŒå¿…è¦
5. **å¤–éƒ¨ä¾å­˜**: GitHub APIãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«æ³¨æ„

### B. è©³ç´°ä»•æ§˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³

#### B.1 æ¤œè¨¼é …ç›®è©³ç´°

##### B.1.1 YAMLè¨­å®šæ¤œè¨¼

```python
def validate_yaml_config(config_path: str, schema_path: str) -> ValidationResult:
    """JSONSchemaã«ã‚ˆã‚‹YAMLæ¤œè¨¼"""
    import yaml
    import jsonschema
    
    # è¨­å®šèª­ã¿è¾¼ã¿
    with open(config_path) as f:
        config = yaml.safe_load(f)
    
    # ã‚¹ã‚­ãƒ¼ãƒèª­ã¿è¾¼ã¿
    with open(schema_path) as f:
        schema = yaml.safe_load(f)
    
    # æ¤œè¨¼å®Ÿè¡Œ
    try:
        jsonschema.validate(config, schema)
        
        # è¿½åŠ ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        errors = []
        
        # global.yamlã¨ã®æ•´åˆæ€§
        if 'tokenizer' in config and config['tokenizer'] != 'fugashi':
            errors.append(f"tokenizer must be 'fugashi', got '{config['tokenizer']}'")
        
        return ValidationResult(
            passed=len(errors) == 0,
            errors=errors,
            warnings=[],
            info={'config_path': config_path}
        )
        
    except jsonschema.ValidationError as e:
        return ValidationResult(
            passed=False,
            errors=[str(e)],
            warnings=[],
            info={'config_path': config_path}
        )
```

##### B.1.2 tokenizerçµ±ä¸€æ€§æ¤œæŸ»

```python
def check_tokenizer_consistency() -> ValidationResult:
    """å…¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§fugashiãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‹æ¤œè¨¼"""
    errors = []
    checked_files = []
    
    # æ¤œæŸ»å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«
    target_patterns = [
        'src/model/jaiml_v3_3/core/utils/tokenize.py',
        'src/vector_pretrainer/scripts/train_tfidf.py',
        'src/lexicon_expansion/scripts/extract_candidates.py'
    ]
    
    for pattern in target_patterns:
        for filepath in glob.glob(pattern, recursive=True):
            with open(filepath) as f:
                content = f.read()
            
            # fugashiä»¥å¤–ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ä½¿ç”¨ã‚’æ¤œå‡º
            if 'MeCab' in content and 'fugashi' not in content:
                errors.append(f"{filepath}: MeCab used without fugashi wrapper")
            
            if 'janome' in content or 'sudachi' in content:
                errors.append(f"{filepath}: Non-fugashi tokenizer detected")
            
            checked_files.append(filepath)
    
    return ValidationResult(
        passed=len(errors) == 0,
        errors=errors,
        warnings=[],
        info={'checked_files': checked_files}
    )
```

##### B.1.3 ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•´åˆæ€§æ¤œæŸ»

```python
def check_version_consistency() -> ValidationResult:
    """sklearnç­‰ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•´åˆæ€§ã‚’æ¤œè¨¼"""
    errors = []
    warnings = []
    
    # metadata.jsonã®ç¢ºèª
    metadata_path = 'model/vectorizers/metadata.json'
    if os.path.exists(metadata_path):
        with open(metadata_path) as f:
            metadata = json.load(f)
        
        # ç¾åœ¨ã®sklearnãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨æ¯”è¼ƒ
        import sklearn
        if metadata.get('sklearn_version') != sklearn.__version__:
            warnings.append(
                f"sklearn version mismatch: "
                f"model={metadata.get('sklearn_version')}, "
                f"current={sklearn.__version__}"
            )
    
    # requirements.txtã¨ã®æ•´åˆæ€§
    requirements_path = 'src/requirements.txt'
    if os.path.exists(requirements_path):
        with open(requirements_path) as f:
            for line in f:
                if 'sklearn' in line:
                    # ãƒãƒ¼ã‚¸ãƒ§ãƒ³æŒ‡å®šã®è§£æ
                    if 'sklearn==1.7.*' not in line:
                        errors.append(
                            f"sklearn version not properly constrained in requirements.txt"
                        )
    
    return ValidationResult(
        passed=len(errors) == 0,
        errors=errors,
        warnings=warnings,
        info={}
    )
```

##### B.1.4 åŒ¿ååŒ–å‡¦ç†æ¤œè¨¼

```python
def check_anonymization_flags(corpus_dir: str) -> ValidationResult:
    """JSONLãƒ•ã‚¡ã‚¤ãƒ«ã®åŒ¿ååŒ–ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œè¨¼"""
    errors = []
    warnings = []
    processed_files = []
    
    for jsonl_file in glob.glob(os.path.join(corpus_dir, '*.jsonl')):
        with open(jsonl_file, 'r', encoding='utf-8') as f:
            for line_no, line in enumerate(f, 1):
                try:
                    data = json.loads(line)
                    metadata = data.get('metadata', {})
                    
                    # å¿…é ˆãƒ•ãƒ©ã‚°ã®ç¢ºèª
                    if not metadata.get('anonymized', False):
                        errors.append(
                            f"{jsonl_file}:{line_no}: Missing or false 'anonymized' flag"
                        )
                    
                    if not metadata.get('verified_by_human', False):
                        warnings.append(
                            f"{jsonl_file}:{line_no}: 'verified_by_human' is false or missing"
                        )
                    
                    # GiNZAãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
                    if 'anonymizer_version' not in metadata:
                        warnings.append(
                            f"{jsonl_file}:{line_no}: Missing 'anonymizer_version'"
                        )
                    
                except json.JSONDecodeError as e:
                    errors.append(f"{jsonl_file}:{line_no}: Invalid JSON - {e}")
        
        processed_files.append(jsonl_file)
    
    return ValidationResult(
        passed=len(errors) == 0,
        errors=errors,
        warnings=warnings,
        info={'processed_files': processed_files}
    )

def scan_unmasked_patterns(corpus_dir: str) -> ValidationResult:
    """æœªãƒã‚¹ã‚¯ã®å€‹äººæƒ…å ±ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º"""
    patterns = {
        'phone': r'\d{2,4}-\d{2,4}-\d{4}',
        'email': r'[\w\.-]+@[\w\.-]+\.\w+',
        'id_number': r'[A-Z]{2,3}\d{6,10}',
        'japanese_name': r'(å±±ç”°|ç”°ä¸­|ä½è—¤|éˆ´æœ¨|é«˜æ©‹|ä¼Šè—¤|æ¸¡è¾º|ä¸­æ‘|å°æ—|åŠ è—¤)[\s]*(å¤ªéƒ|èŠ±å­|ä¸€éƒ|äºŒéƒ|ä¸‰éƒ)',
        'credit_card': r'\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}',
        'postal_code': r'ã€’?\d{3}-\d{4}'
    }
    
    warnings = []
    detected_patterns = []
    
    for jsonl_file in glob.glob(os.path.join(corpus_dir, '*.jsonl')):
        with open(jsonl_file, 'r', encoding='utf-8') as f:
            for line_no, line in enumerate(f, 1):
                try:
                    data = json.loads(line)
                    text_fields = [
                        data.get('user', ''),
                        data.get('response', ''),
                        data.get('text', '')
                    ]
                    
                    for field_text in text_fields:
                        if not field_text:
                            continue
                            
                        for pattern_name, pattern in patterns.items():
                            matches = re.findall(pattern, field_text)
                            if matches:
                                for match in matches:
                                    warnings.append(
                                        f"{jsonl_file}:{line_no}: "
                                        f"Potential {pattern_name} found: '{match}'"
                                    )
                                    detected_patterns.append({
                                        'file': jsonl_file,
                                        'line': line_no,
                                        'type': pattern_name,
                                        'content': match
                                    })
                
                except json.JSONDecodeError:
                    continue  # JSONã‚¨ãƒ©ãƒ¼ã¯åˆ¥ã®æ¤œè¨¼ã§å‡¦ç†
    
    return ValidationResult(
        passed=True,  # è­¦å‘Šã®ã¿ã€ã‚¨ãƒ©ãƒ¼ã¨ã—ãªã„
        errors=[],
        warnings=warnings,
        info={'detected_patterns': detected_patterns}
    )
```

##### B.1.5 çµ±åˆåŒ¿ååŒ–æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```python
# ci/check_anonymization.py
import argparse
import json
import sys
from typing import Dict, Any

def main():
    parser = argparse.ArgumentParser(description='Check anonymization in JSONL files')
    parser.add_argument('--corpus-dir', required=True, help='Directory containing JSONL files')
    parser.add_argument('--output-format', choices=['json', 'text'], default='text')
    args = parser.parse_args()
    
    # ãƒ•ãƒ©ã‚°æ¤œè¨¼
    flag_result = check_anonymization_flags(args.corpus_dir)
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º
    pattern_result = scan_unmasked_patterns(args.corpus_dir)
    
    # çµæœã®çµ±åˆ
    overall_passed = flag_result.passed and len(pattern_result.warnings) == 0
    
    report = {
        'timestamp': datetime.utcnow().isoformat() + 'Z',
        'corpus_dir': args.corpus_dir,
        'overall_passed': overall_passed,
        'flag_check': {
            'passed': flag_result.passed,
            'errors': flag_result.errors,
            'warnings': flag_result.warnings
        },
        'pattern_scan': {
            'warnings': pattern_result.warnings,
            'detected_patterns': pattern_result.info.get('detected_patterns', [])
        }
    }
    
    if args.output_format == 'json':
        print(json.dumps(report, indent=2, ensure_ascii=False))
    else:
        print(f"Anonymization Check Report - {report['timestamp']}")
        print(f"Corpus Directory: {report['corpus_dir']}")
        print(f"Overall Result: {'PASSED' if overall_passed else 'FAILED'}")
        
        if flag_result.errors:
            print("\nFlag Check Errors:")
            for error in flag_result.errors:
                print(f"  - {error}")
        
        if pattern_result.warnings:
            print(f"\nPotential Unmasked Information ({len(pattern_result.warnings)} found):")
            for warning in pattern_result.warnings[:10]:  # æœ€åˆã®10ä»¶ã®ã¿è¡¨ç¤º
                print(f"  - {warning}")
            if len(pattern_result.warnings) > 10:
                print(f"  ... and {len(pattern_result.warnings) - 10} more")
    
    # CIçµ‚äº†ã‚³ãƒ¼ãƒ‰
    sys.exit(0 if flag_result.passed else 1)

if __name__ == '__main__':
    main()
```

##### B.1.6 è¾æ›¸ãƒãƒƒã‚·ãƒ¥æ•´åˆæ€§æ¤œæŸ»

```python
def check_lexicon_hash_consistency() -> ValidationResult:
    """è¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã¨changelogã®ãƒãƒƒã‚·ãƒ¥æ•´åˆæ€§ã‚’æ¤œè¨¼"""
    errors = []
    warnings = []
    
    changelog_path = 'lexicons/versions/changelog.json'
    if not os.path.exists(changelog_path):
        return ValidationResult(
            passed=False,
            errors=[f"Changelog not found: {changelog_path}"],
            warnings=[],
            info={}
        )
    
    with open(changelog_path, 'r', encoding='utf-8') as f:
        changelog = json.load(f)
    
    for version in changelog.get('versions', []):
        timestamp = version['timestamp']
        expected_hash = version.get('file_hash')
        
        if not expected_hash:
            warnings.append(f"Version {timestamp}: Missing file_hash in changelog")
            continue
        
        # å¯¾å¿œã™ã‚‹è¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
        lexicon_file = f'lexicons/versions/jaiml_lexicons_{timestamp}.yaml'
        if os.path.exists(lexicon_file):
            # å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—
            with open(lexicon_file, 'rb') as f:
                actual_hash = 'sha256:' + hashlib.sha256(f.read()).hexdigest()
            
            if actual_hash != expected_hash:
                errors.append(
                    f"Hash mismatch for {lexicon_file}: "
                    f"expected={expected_hash}, actual={actual_hash}"
                )
    
    return ValidationResult(
        passed=len(errors) == 0,
        errors=errors,
        warnings=warnings,
        info={'versions_checked': len(changelog.get('versions', []))}
    )
```

##### B.1.7 ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°å“è³ªæ¤œè¨¼

```python
def check_clustering_quality(metrics_file: str) -> ValidationResult:
    """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è©•ä¾¡æŒ‡æ¨™ã®æ¤œè¨¼"""
    errors = []
    warnings = []
    
    if not os.path.exists(metrics_file):
        return ValidationResult(
            passed=False,
            errors=[f"Metrics file not found: {metrics_file}"],
            warnings=[],
            info={}
        )
    
    with open(metrics_file, 'r', encoding='utf-8') as f:
        metrics = json.load(f)
    
    silhouette_score = metrics.get('silhouette_score')
    if silhouette_score is None:
        errors.append("Missing silhouette_score in metrics")
    elif silhouette_score < 0.25:
        warnings.append(
            f"Low silhouette score: {silhouette_score:.3f} (threshold: 0.25). "
            "This indicates poor clustering quality."
        )
    
    # JSONã‚¹ã‚­ãƒ¼ãƒæ¤œè¨¼
    required_fields = ['algorithm', 'n_clusters', 'silhouette_score', 'timestamp']
    missing_fields = [field for field in required_fields if field not in metrics]
    if missing_fields:
        errors.append(f"Missing required fields: {', '.join(missing_fields)}")
    
    return ValidationResult(
        passed=len(errors) == 0,
        errors=errors,
        warnings=warnings,
        info=metrics
    )
```

##### B.1.8 TF-IDFå†ç¾æ€§ãƒ†ã‚¹ãƒˆ

```python
def test_tfidf_reproducibility() -> ValidationResult:
    """TF-IDF noveltyã‚¹ã‚³ã‚¢ã®å†ç¾æ€§ã‚’æ¤œè¨¼"""
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    test_cases = [
            # çœç•¥ã•ã‚ŒãŸå®Ÿè£…ã‚’ã“ã“ã«è¿½åŠ 
    ]
```
##### B.1.9 ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å“è³ªç›£è¦–

```python
# ci/check_annotation_metrics.py
import json
import os
import numpy as np
from sklearn.metrics import cohen_kappa_score, f1_score
from typing import Dict, List, Tuple
import warnings

def calculate_weighted_kappa(y1: List[int], y2: List[int]) -> float:
    """Calculate quadratic weighted kappa"""
    return cohen_kappa_score(y1, y2, weights='quadratic')

def calculate_macro_f1(y_true: List[int], y_pred: List[int], n_classes: int = 5) -> float:
    """Calculate macro F1 score for ordinal labels"""
    return f1_score(y_true, y_pred, labels=list(range(1, n_classes+1)), average='macro')

def check_annotation_quality(annotation_file: str) -> ValidationResult:
    """Check annotation quality metrics"""
    errors = []
    warnings = []
    metrics = {}
    
    if not os.path.exists(annotation_file):
        return ValidationResult(
            passed=False,
            errors=[f"Annotation file not found: {annotation_file}"],
            warnings=[],
            info={}
        )
    
    # Load annotations
    with open(annotation_file, 'r', encoding='utf-8') as f:
        annotations = json.load(f)
    
    # Calculate metrics for each axis
    axes = ['social', 'avoidant', 'mechanical', 'self']
    
    for axis in axes:
        # Extract annotations for this axis
        annotator1_scores = []
        annotator2_scores = []
        
        for item in annotations.get('items', []):
            if axis in item.get('annotations', {}):
                scores = item['annotations'][axis]
                if len(scores) >= 2:
                    annotator1_scores.append(scores[0])
                    annotator2_scores.append(scores[1])
        
        if len(annotator1_scores) < 10:
            warnings.append(f"{axis}: Insufficient annotations ({len(annotator1_scores)} samples)")
            continue
        
        # Calculate metrics
        kappa = calculate_weighted_kappa(annotator1_scores, annotator2_scores)
        
        # For F1, we need predicted vs true labels
        # Here we use annotator consensus as "true" label
        consensus_scores = [
            round(np.mean([s1, s2])) 
            for s1, s2 in zip(annotator1_scores, annotator2_scores)
        ]
        
        # Simulate predictions for F1 calculation (in real scenario, use model predictions)
        # For CI check, we calculate inter-annotator F1
        f1 = calculate_macro_f1(consensus_scores, annotator1_scores)
        
        metrics[axis] = {
            'weighted_kappa': kappa,
            'macro_f1': f1,
            'n_samples': len(annotator1_scores)
        }
        
        # Check thresholds
        if kappa < 0.60:
            warnings.append(
                f"{axis}: Low inter-annotator agreement (Îº={kappa:.3f} < 0.60)"
            )
        
        if f1 < 0.60:
            warnings.append(
                f"{axis}: Low F1 score (F1={f1:.3f} < 0.60)"
            )
    
    # Overall metrics
    overall_kappa = np.mean([m['weighted_kappa'] for m in metrics.values()])
    overall_f1 = np.mean([m['macro_f1'] for m in metrics.values()])
    
    metrics['overall'] = {
        'mean_weighted_kappa': overall_kappa,
        'mean_macro_f1': overall_f1
    }
    
    return ValidationResult(
        passed=len(errors) == 0,
        errors=errors,
        warnings=warnings,
        info={'metrics': metrics}
    )

def main():
    """Main entry point for CI"""
    import argparse
    import sys
    
    parser = argparse.ArgumentParser(description='Check annotation quality metrics')
    parser.add_argument('--annotation-file', default='corpus/annotations/pilot_annotations.json')
    parser.add_argument('--output-format', choices=['json', 'text'], default='text')
    args = parser.parse_args()
    
    result = check_annotation_quality(args.annotation_file)
    
    if args.output_format == 'json':
        report = {
            'passed': result.passed,
            'errors': result.errors,
            'warnings': result.warnings,
            'metrics': result.info.get('metrics', {})
        }
        print(json.dumps(report, indent=2, ensure_ascii=False))
    else:
        print("Annotation Quality Check")
        print("=" * 50)
        
        if result.errors:
            print("\nERRORS:")
            for error in result.errors:
                print(f"  - {error}")
        
        if result.warnings:
            print("\nWARNINGS:")
            for warning in result.warnings:
                print(f"  - {warning}")
        
        metrics = result.info.get('metrics', {})
        if metrics:
            print("\nMETRICS:")
            for axis, values in metrics.items():
                if axis != 'overall':
                    print(f"\n  {axis.upper()}:")
                    print(f"    Weighted Îº: {values['weighted_kappa']:.3f}")
                    print(f"    Macro F1:   {values['macro_f1']:.3f}")
                    print(f"    Samples:    {values['n_samples']}")
            
            if 'overall' in metrics:
                print(f"\n  OVERALL:")
                print(f"    Mean Îº:  {metrics['overall']['mean_weighted_kappa']:.3f}")
                print(f"    Mean F1: {metrics['overall']['mean_macro_f1']:.3f}")
    
    sys.exit(0 if result.passed and not result.warnings else 1)

if __name__ == '__main__':
    main()
```

#### B.1.10 CIæ¤œè¨¼é …ç›®ãƒ†ãƒ¼ãƒ–ãƒ«

| æ¤œè¨¼é …ç›® | å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ | æ¤œè¨¼å†…å®¹ | å¤±æ•—æ¡ä»¶ |
|----------|---------------|----------|----------|
| ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å“è³ª | `check_annotation_metrics.py` | è©•ä¾¡è€…é–“ä¸€è‡´ç‡ã¨F1ã‚¹ã‚³ã‚¢ | Îº<0.60 or Macro-F1<0.60 |
| ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®Œå…¨æ€§ | `check_annotation_metrics.py` | å¿…é ˆè»¸ã®è©•ä¾¡å­˜åœ¨ | 4è»¸ã„ãšã‚Œã‹ã®æ¬ æ |
| è©•ä¾¡è€…æ•° | `check_annotation_metrics.py` | æœ€å°è©•ä¾¡è€…æ•°ç¢ºèª | <3äºº/å¯¾è©± |
| æ™‚ç³»åˆ—ä¸€è²«æ€§ | `check_annotation_metrics.py` | è©•ä¾¡æ™‚æœŸã®å¦¥å½“æ€§ | æœªæ¥æ—¥ä»˜ã®æ¤œå‡º |
| ãƒ­ã‚°ã‚¹ã‚­ãƒ¼ãƒæº–æ‹ æ€§ | `validate_annotation_logs.py` | JSONã‚¹ã‚­ãƒ¼ãƒæ¤œè¨¼ | ã‚¹ã‚­ãƒ¼ãƒé•å |
| ãƒ­ã‚°æ™‚åˆ»è«–ç†æ€§ | `validate_annotation_logs.py` | ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—æ•´åˆæ€§ | å·®åˆ†>0.1ç§’ |
| UIå®šç¾©æ•´åˆæ€§ | `check_ui_consistency.py` | ãƒ©ãƒ™ãƒ«ãƒ»ã‚¹ã‚±ãƒ¼ãƒ«ä¸€è‡´ | ä¸ä¸€è‡´æ¤œå‡º |

##### B.1.11 Nightly Annotation Quality Job

```yaml
# .github/workflows/annotation_quality.yml
name: Annotation Quality Check

on:
  schedule:
    - cron: '0 2 * * *'  # æ¯æ—¥åˆå‰2æ™‚ï¼ˆUTCï¼‰
  workflow_dispatch:  # æ‰‹å‹•å®Ÿè¡Œã‚‚å¯èƒ½

jobs:
  check-quality:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install numpy scikit-learn
    
    - name: Check annotation metrics
      run: |
        python ci/check_annotation_metrics.py \
          --annotation-file corpus/annotations/pilot_annotations.json \
          --output-format json > annotation_metrics.json
    
     - name: Validate annotation logs
      run: python src/ci/validate_annotation_logs.py --log-dir annotation/logs/current/
    
    - name: Check UI consistency
      run: python src/ci/check_ui_consistency.py --snapshot-file ci/snapshots/ui_spec.json   
    
    - name: Upload metrics report
      uses: actions/upload-artifact@v3
      with:
        name: annotation-metrics
        path: annotation_metrics.json
    
    - name: Post to Slack on warnings
      if: failure()
      uses: 8398a7/action-slack@v3
      with:
        status: ${{ job.status }}
        text: 'Annotation quality check detected issues. Please review the metrics.'
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}
```

##### B.1.12 ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°æ¤œè¨¼

```python
def validate_annotation_logs(log_dir: str) -> ValidationResult:
    """ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°ã®JSONã‚¹ã‚­ãƒ¼ãƒæº–æ‹ æ€§ã‚’æ¤œè¨¼"""
    errors = []
    warnings = []
    
    schema = {
        "type": "object",
        "required": [
            "annotator_id", "target_id", "scores", 
            "timestamp_start", "timestamp_end", "annotation_duration",
            "is_modified", "browser_meta", "confidence_flag"
        ],
        "properties": {
            "annotator_id": {"type": "string", "pattern": "^[a-f0-9]{64}$"},
            "target_id": {"type": "string", "format": "uuid"},
            "scores": {
                "type": "object",
                "required": ["social", "avoidant", "mechanical", "self"],
                "properties": {
                    "social": {"type": "integer", "minimum": 0, "maximum": 4},
                    "avoidant": {"type": "integer", "minimum": 0, "maximum": 4},
                    "mechanical": {"type": "integer", "minimum": 0, "maximum": 4},
                    "self": {"type": "integer", "minimum": 0, "maximum": 4}
                }
            }
        }
    }
    
    for log_file in glob.glob(os.path.join(log_dir, "*.jsonl")):
        with open(log_file, 'r') as f:
            for line_no, line in enumerate(f, 1):
                try:
                    record = json.loads(line)
                    jsonschema.validate(record, schema)
                    
                    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è«–ç†æ€§ãƒã‚§ãƒƒã‚¯
                    start = datetime.fromisoformat(record['timestamp_start'])
                    end = datetime.fromisoformat(record['timestamp_end'])
                    duration = record['annotation_duration']
                    
                    calculated = (end - start).total_seconds()
                    if abs(calculated - duration) > 0.1:
                        warnings.append(
                            f"{log_file}:{line_no}: Duration mismatch "
                            f"(calculated: {calculated}, recorded: {duration})"
                        )
                        
                except json.JSONDecodeError as e:
                    errors.append(f"{log_file}:{line_no}: Invalid JSON - {e}")
                except jsonschema.ValidationError as e:
                    errors.append(f"{log_file}:{line_no}: Schema violation - {e}")
    
    return ValidationResult(
        passed=len(errors) == 0,
        errors=errors,
        warnings=warnings,
        info={'files_checked': len(list(glob.glob(os.path.join(log_dir, "*.jsonl"))))}
    )
```

##### B.1.13 UIæ•´åˆæ€§ãƒã‚§ãƒƒã‚¯

```python
def check_ui_consistency(snapshot_file: str = "ci/snapshots/ui_spec.json") -> ValidationResult:
    """UIè¦ç´ ã¨ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å®šç¾©ã®æ•´åˆæ€§ã‚’æ¤œè¨¼"""
    errors = []
    
    # UIã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆèª­ã¿è¾¼ã¿
    if not os.path.exists(snapshot_file):
        return ValidationResult(
            passed=False,
            errors=[f"UI snapshot not found: {snapshot_file}"],
            warnings=[],
            info={}
        )
    
    with open(snapshot_file, 'r') as f:
        ui_spec = json.load(f)
    
    # æœŸå¾…ã•ã‚Œã‚‹è¦ç´ 
    expected_axes = ['social', 'avoidant', 'mechanical', 'self']
    expected_labels = {
        'social': 'ç¤¾ä¼šçš„è¿åˆ',
        'avoidant': 'å›é¿çš„è¿åˆ',
        'mechanical': 'æ©Ÿæ¢°çš„è¿åˆ',
        'self': 'è‡ªå·±è¿åˆ'
    }
    expected_scale = {
        0: 'ãªã—',
        1: 'ã‚ãšã‹',
        2: 'ä¸­ç¨‹åº¦',
        3: 'å¼·ã„',
        4: 'æ¥µåº¦'
    }
    
    # UIã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã¨ã®ç…§åˆ
    ui_axes = ui_spec.get('score_inputs', {})
    for axis in expected_axes:
        if axis not in ui_axes:
            errors.append(f"Missing UI element for axis: {axis}")
        else:
            # ãƒ©ãƒ™ãƒ«ãƒã‚§ãƒƒã‚¯
            if ui_axes[axis].get('label') != expected_labels[axis]:
                errors.append(
                    f"Label mismatch for {axis}: "
                    f"expected '{expected_labels[axis]}', "
                    f"got '{ui_axes[axis].get('label')}'"
                )
            
            # ã‚¹ã‚±ãƒ¼ãƒ«ãƒã‚§ãƒƒã‚¯
            ui_options = ui_axes[axis].get('options', {})
            for value, label in expected_scale.items():
                if str(value) not in ui_options or ui_options[str(value)] != label:
                    errors.append(
                        f"Scale mismatch for {axis}[{value}]: "
                        f"expected '{label}', got '{ui_options.get(str(value))}'"
                    )
    
    return ValidationResult(
        passed=len(errors) == 0,
        errors=errors,
        warnings=[],
        info={'snapshot_timestamp': ui_spec.get('timestamp')}
    )
```

#### B.2 ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ¤œæŸ»

##### B.2.1 Pickleä½¿ç”¨æ¤œå‡º

```python
def scan_pickle_usage() -> ValidationResult:
    """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒªã‚¹ã‚¯ã®ã‚ã‚‹Pickleä½¿ç”¨ã‚’æ¤œå‡º"""
    errors = []
    warnings = []
    pickle_usage = []
    
    for root, dirs, files in os.walk('src'):
        # __pycache__ã‚’ã‚¹ã‚­ãƒƒãƒ—
        dirs[:] = [d for d in dirs if d != '__pycache__']
        
        for file in files:
            if file.endswith('.py'):
                filepath = os.path.join(root, file)
                with open(filepath) as f:
                    content = f.read()
                
                # pickleä½¿ç”¨ã®æ¤œå‡º
                if 'import pickle' in content or 'from pickle' in content:
                    # è­¦å‘Šã‚³ãƒ¡ãƒ³ãƒˆã®ç¢ºèª
                    if 'trusted source only' not in content.lower():
                        errors.append(
                            f"{filepath}: Pickle usage without security warning"
                        )
                    else:
                        warnings.append(
                            f"{filepath}: Pickle usage with warning (consider joblib)"
                        )
                    
                    pickle_usage.append(filepath)
    
    return ValidationResult(
        passed=len(errors) == 0,
        errors=errors,
        warnings=warnings,
        info={'pickle_files': pickle_usage}
    )
```

##### B.2.2 ä¾å­˜é–¢ä¿‚è„†å¼±æ€§æ¤œæŸ»

```python
def check_dependency_vulnerabilities() -> ValidationResult:
    """æ—¢çŸ¥ã®è„†å¼±æ€§ã‚’æŒã¤ä¾å­˜é–¢ä¿‚ã‚’æ¤œå‡º"""
    import subprocess
    
    try:
        # safetyã«ã‚ˆã‚‹è„†å¼±æ€§æ¤œæŸ»
        result = subprocess.run(
            ['safety', 'check', '--json'],
            capture_output=True,
            text=True
        )
        
        vulnerabilities = json.loads(result.stdout)
        
        if vulnerabilities:
            errors = [
                f"{v['package']}: {v['vulnerability']}" 
                for v in vulnerabilities
            ]
            return ValidationResult(
                passed=False,
                errors=errors,
                warnings=[],
                info={'vulnerabilities': vulnerabilities}
            )
        
        return ValidationResult(
            passed=True,
            errors=[],
            warnings=[],
            info={'message': 'No vulnerabilities found'}
        )
        
    except Exception as e:
        return ValidationResult(
            passed=False,
            errors=[f"Failed to run safety check: {e}"],
            warnings=[],
            info={}
        )
```

#### B.3 GitHub Actionsè¨­å®š

##### B.3.1 validate.yml

```yaml
name: Validate Configuration

on:
  push:
    paths:
      - 'config/*.yaml'
      - 'src/ci/**'
  pull_request:
    paths:
      - 'config/*.yaml'
      - 'src/ci/**'

jobs:
  validate:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install -r src/requirements.txt
        pip install jsonschema pyyaml
    
    - name: Validate YAML configs
      run: python src/ci/schema_validate.py
    
    - name: Check tokenizer consistency
      run: python src/ci/check_tokenizer.py
    
    - name: Check version consistency
      run: python src/ci/check_versions.py

    - name: Check anonymization
      run: python src/ci/check_anonymization.py --corpus-dir corpus/jsonl/

    - name: Check annotation metrics
      run: python src/ci/check_annotation_metrics.py --annotation-file corpus/annotations/pilot_annotations.json
    
    - name: Generate report
      if: always()
      run: |
        python src/ci/run_all_checks.py --output-format markdown > validation_report.md
    
    - name: Upload report
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: validation-report
        path: validation_report.md
```

##### B.3.2 security.yml

```yaml
name: Security Audit

on:
  schedule:
    - cron: '0 0 * * 1'  # æ¯é€±æœˆæ›œæ—¥
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  security:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install security tools
      run: |
        pip install bandit safety
    
    - name: Run bandit
      run: bandit -r src/ -f json -o bandit_report.json
    
    - name: Run safety check
      run: safety check --json > safety_report.json
    
    - name: Check for pickle usage
      run: python src/ci/check_security.py
    
    - name: Upload security reports
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: security-reports
        path: |
          bandit_report.json
          safety_report.json
```

#### B.4 cluster_metrics.json ã‚¹ã‚­ãƒ¼ãƒ

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "title": "Cluster Metrics Report",
  "type": "object",
  "required": ["timestamp", "algorithm", "n_clusters", "silhouette_score", "clusters"],
  "properties": {
    "timestamp": {
      "type": "string",
      "format": "date-time",
      "description": "ISO 8601 timestamp of report generation"
    },
    "algorithm": {
      "type": "string",
      "enum": ["k-means", "dbscan", "hierarchical"],
      "description": "Clustering algorithm used"
    },
    "n_clusters": {
      "type": "integer",
      "minimum": 2,
      "description": "Number of clusters identified"
    },
    "silhouette_score": {
      "type": "number",
      "minimum": -1,
      "maximum": 1,
      "description": "Overall silhouette coefficient"
    },
    "clusters": {
      "type": "array",
      "items": {
        "type": "object",
        "required": ["id", "size", "silhouette_score", "representative_terms"],
        "properties": {
          "id": {
            "type": "integer",
            "minimum": 0
          },
          "size": {
            "type": "integer",
            "minimum": 1,
            "description": "Number of terms in cluster"
          },
          "silhouette_score": {
            "type": "number",
            "minimum": -1,
            "maximum": 1,
            "description": "Cluster-specific silhouette score"
          },
          "representative_terms": {
            "type": "array",
            "items": {
              "type": "string"
            },
            "minItems": 1,
            "maxItems": 10,
            "description": "Most representative terms in cluster"
          },
          "annotation_difficulty": {
            "type": "string",
            "enum": ["low", "medium", "high"],
            "description": "Estimated annotation difficulty based on silhouette score"
          }
        }
      }
    }
  }
}
```

#### B.5 changelog.json ãƒãƒƒã‚·ãƒ¥å¿…é ˆåŒ–

```python
def validate_changelog_hash(changelog_path: str) -> ValidationResult:
    """Validate that all changelog entries have file_hash"""
    errors = []
    warnings = []
    
    with open(changelog_path, 'r', encoding='utf-8') as f:
        changelog = json.load(f)
    
    for idx, version in enumerate(changelog.get('versions', [])):
        if 'file_hash' not in version:
            errors.append(
                f"Version entry {idx} (timestamp: {version.get('timestamp', 'unknown')}): "
                f"Missing required 'file_hash' field"
            )
        elif not version['file_hash'].startswith('sha256:'):
            warnings.append(
                f"Version entry {idx}: file_hash should start with 'sha256:' prefix"
            )
    
    return ValidationResult(
        passed=len(errors) == 0,
        errors=errors,
        warnings=warnings,
        info={'total_versions': len(changelog.get('versions', []))}
    )
```

#### B.6 UIã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä»•æ§˜

```json
// ci/snapshots/ui_spec.json
{
  "timestamp": "2025-01-15T10:00:00Z",
  "version": "1.0",
  "score_inputs": {
    "social": {
      "label": "ç¤¾ä¼šçš„è¿åˆ",
      "options": {
        "0": "ãªã—",
        "1": "ã‚ãšã‹",
        "2": "ä¸­ç¨‹åº¦",
        "3": "å¼·ã„",
        "4": "æ¥µåº¦"
      }
    },
    "avoidant": { ... },
    "mechanical": { ... },
    "self": { ... }
  },
  "confidence_options": {
    "certain": "ç¢ºä¿¡",
    "uncertain": "ä¸ç¢ºä¿¡"
  },
  "required_elements": ["dialogue_history", "current_pair", "timer_display"]
}
```
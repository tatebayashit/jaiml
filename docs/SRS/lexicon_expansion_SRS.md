## ğŸ“š lexicon_expansion v2.0 ã‚·ã‚¹ãƒ†ãƒ è¦æ±‚ä»•æ§˜æ›¸ - æ”¹è¨‚ç‰ˆ

### A. çµ±ä¸€è¨˜è¿°ã‚»ã‚¯ã‚·ãƒ§ãƒ³

#### A.1 æ¦‚è¦

**ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å**: lexicon_expansion v2.0

**ç›®çš„**: JAIML v3.3ã«ãŠã‘ã‚‹èªå½™è¾æ›¸ï¼ˆ`lexicons/jaiml_lexicons.yaml`ï¼‰ã®ç³»çµ±çš„æ‹¡å¼µã¨ã€è¾æ›¸ãƒ™ãƒ¼ã‚¹ã®å¼±æ•™å¸«ä»˜ãå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆã‚’å®Ÿç¾ã™ã‚‹çµ±åˆåŸºç›¤ã€‚ã‚³ãƒ¼ãƒ‘ã‚¹ã‹ã‚‰ã®è‡ªå‹•æŠ½å‡ºã€äººæ‰‹æ¤œè¨¼ã€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã€è‡ªå‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®4æ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹ã€‚

#### A.2 ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹æˆã¨è²¬å‹™

```
src/lexicon_expansion/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ extraction_rules.yaml     # æŠ½å‡ºãƒ«ãƒ¼ãƒ«å®šç¾©
â”‚   â””â”€â”€ category_schemas.yaml     # ã‚«ãƒ†ã‚´ãƒªã‚¹ã‚­ãƒ¼ãƒ
â”œâ”€â”€ corpus/                       # å…¥åŠ›ã‚³ãƒ¼ãƒ‘ã‚¹
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_expansion.py          # ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
â”‚   â”œâ”€â”€ extract_candidates.py     # å€™è£œæŠ½å‡º
â”‚   â”œâ”€â”€ merge_lexicons.py         # è¾æ›¸çµ±åˆ
â”‚   â””â”€â”€ validate_yaml.py          # æ¤œè¨¼
â”œâ”€â”€ annotation/
â”‚   â”œâ”€â”€ auto_annotator.py         # è‡ªå‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
â”‚   â”œâ”€â”€ tfidf_novelty_calc.py     # TF-IDFæ–°è¦æ€§è¨ˆç®—
â”‚   â””â”€â”€ snippet_generator.py      # ã‚¹ãƒ‹ãƒšãƒƒãƒˆç”Ÿæˆ
â”œâ”€â”€ clustering/
â”‚   â”œâ”€â”€ semantic_clustering.py    # æ„å‘³çš„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°
â”‚   â””â”€â”€ overexpression_detector.py # éå‰°è¡¨ç¾æ¤œå‡º
â”œâ”€â”€ version_control/
â”‚   â”œâ”€â”€ version_manager.py        # ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†
â”‚   â””â”€â”€ trend_analyzer.py         # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
â””â”€â”€ outputs/
    â”œâ”€â”€ candidates/               # æŠ½å‡ºå€™è£œ
    â”œâ”€â”€ reports/                  # ãƒ¬ãƒãƒ¼ãƒˆ
    â”‚   â””â”€â”€ cluster_metrics.json  # ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è©•ä¾¡æŒ‡æ¨™
    â””â”€â”€ snippets/                 # ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çµæœ
```

**è²¬å‹™**:
- ã‚³ãƒ¼ãƒ‘ã‚¹ã‹ã‚‰ã®èªå½™å€™è£œè‡ªå‹•æŠ½å‡º
- äººæ‰‹æ¤œè¨¼ãƒ—ãƒ­ã‚»ã‚¹ã®æ”¯æ´
- è¾æ›¸ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã¨å·®åˆ†è¿½è·¡
- å¼±æ•™å¸«ä»˜ãå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ

#### A.3 å…¥å‡ºåŠ›ä»•æ§˜

**å…¥åŠ›å½¢å¼**:
1. ã‚³ãƒ¼ãƒ‘ã‚¹ï¼ˆJSONL/ãƒ—ãƒ¬ãƒ¼ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼‰
2. æ—¢å­˜è¾æ›¸ï¼ˆ`jaiml_lexicons.yaml`ï¼‰
3. æŠ½å‡ºãƒ«ãƒ¼ãƒ«ï¼ˆ`extraction_rules.yaml`ï¼‰

**å‡ºåŠ›å½¢å¼**:
1. æ‹¡å¼µè¾æ›¸ï¼ˆ`jaiml_lexicons_TIMESTAMP.yaml`ï¼‰
2. å¼±æ•™å¸«ãƒ‡ãƒ¼ã‚¿ï¼ˆJSONLå½¢å¼ï¼‰
3. å¤‰æ›´ãƒ­ã‚°ï¼ˆ`changelog.json`ï¼‰
4. ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆï¼ˆ`cluster_metrics.json`ï¼‰

#### A.4 ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å®šç¾©

**å…±é€šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆconfig/global.yamlã‹ã‚‰ç¶™æ‰¿ï¼‰**:
- `tokenizer`: "fugashi"
- `encoding`: "utf-8"
- `lexicon_path`: "lexicons/jaiml_lexicons.yaml"

**è¾æ›¸æ‹¡å¼µå°‚ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- `min_frequency`: 5ï¼ˆæœ€å°å‡ºç¾é »åº¦ï¼‰
- `ngram_range`: [1, 7]ï¼ˆN-gramç¯„å›²ï¼‰
- `context_window`: 50ï¼ˆã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ–‡è„ˆçª“ï¼‰
- `novelty_top_k`: 0.2ï¼ˆTF-IDFæ–°è¦æ€§ä¸Šä½20%é–¾å€¤ï¼‰
- `context_window_unit`: "morphemes"ï¼ˆå½¢æ…‹ç´ æ•°å˜ä½ã€å€¤åŸŸ[10, 200]

#### A.5 é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
lexicons/
â”œâ”€â”€ jaiml_lexicons.yaml          # ãƒã‚¹ã‚¿ãƒ¼è¾æ›¸ï¼ˆ11ã‚«ãƒ†ã‚´ãƒªå¿…é ˆï¼‰
â””â”€â”€ versions/                    # ãƒãƒ¼ã‚¸ãƒ§ãƒ³å±¥æ­´
    â”œâ”€â”€ jaiml_lexicons_TIMESTAMP.yaml
    â””â”€â”€ changelog.json

config/
â”œâ”€â”€ global.yaml                  # å…±é€šè¨­å®š
â”œâ”€â”€ category_schemas.yaml        # ã‚«ãƒ†ã‚´ãƒªã‚¹ã‚­ãƒ¼ãƒå®šç¾©
â”œâ”€â”€ extraction_rules.yaml        # æŠ½å‡ºãƒ«ãƒ¼ãƒ«
```

#### A.6 ä½¿ç”¨ä¾‹ã¨ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³

**å€™è£œæŠ½å‡º**:
```bash
python run_expansion.py \
  --phase extract \
  --corpus corpus.jsonl \
  --categories template_phrases humble_phrases
```

**æ¤œè¨¼ã¨çµ±åˆ**:
```bash
# å€™è£œã®æ¤œè¨¼
python run_expansion.py --phase validate

# è¾æ›¸ã¸ã®çµ±åˆ
python run_expansion.py --phase merge --output lexicons/
```

**è‡ªå‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³**:
```bash
python run_advanced_features.py \
  --feature annotate \
  --corpus dialogue.jsonl \
  --output outputs/weak_supervised.jsonl
```

#### A.7 CIæ¤œè¨¼é …ç›®

1. **è¾æ›¸å®Œå…¨æ€§**: 11ã‚«ãƒ†ã‚´ãƒªã™ã¹ã¦ãŒå­˜åœ¨ã™ã‚‹ã“ã¨
2. **YAMLå¦¥å½“æ€§**: `yaml.safe_load()`ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãªã„ã“ã¨
3. **é‡è¤‡æ¤œæŸ»**: å„ã‚«ãƒ†ã‚´ãƒªå†…ã«é‡è¤‡ã‚¨ãƒ³ãƒˆãƒªãŒãªã„ã“ã¨
4. **æ­£è¦åŒ–ä¸€è²«æ€§**: Unicode NFKCæ­£è¦åŒ–ãŒé©ç”¨ã•ã‚Œã¦ã„ã‚‹ã“ã¨
5. **ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ•´åˆæ€§**: changelogã¨å®Ÿéš›ã®å·®åˆ†ãŒä¸€è‡´ã™ã‚‹ã“ã¨
6. **canonical_keyæ•´åˆæ€§**: å…¨ã‚¨ãƒ³ãƒˆãƒªã®canonical_keyãŒæ­£è¦åŒ–ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ã„ã‚‹ã“ã¨

#### A.8 ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å®šç¾©ï¼ˆå‹æ³¨é‡ˆä»˜ãï¼‰

```python
from typing import Dict, List, Set, Optional, Tuple
from collections import OrderedDict

class CandidateExtractor:
    def __init__(self, config_path: str):
        """
        Args:
            config_path: extraction_rules.yamlã®ãƒ‘ã‚¹
        """
    
    def extract_category(self, corpus_path: str, category: str) -> Dict[str, Dict]:
        """
        ç‰¹å®šã‚«ãƒ†ã‚´ãƒªã®å€™è£œã‚’æŠ½å‡º
        
        Returns:
            Dict[str, Dict]: {
                "phrase": {
                    "frequency": int,
                    "pos_patterns": List[str]
                }
            }
        """

class LexiconMatcher:
    def __init__(self, lexicon_path: str):
        """è¾æ›¸ã®èª­ã¿è¾¼ã¿"""
    
    def match(self, text: str) -> OrderedDict[str, List[str]]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã«å¯¾ã™ã‚‹è¾æ›¸ãƒãƒƒãƒãƒ³ã‚°
        
        Returns:
            OrderedDict: ã‚«ãƒ†ã‚´ãƒªé †åºå›ºå®šã®è¾æ›¸
        """

class AutoAnnotator:
    def __init__(self, lexicon_path: str):
        """è‡ªå‹•ã‚¢ãƒãƒ†ãƒ¼ã‚¿ãƒ¼ã®åˆæœŸåŒ–"""
        self.lexicon_matcher = LexiconMatcher(lexicon_path)
        self.novelty_calculator = TFIDFNoveltyCalculator()
     
    def calculate_novelty_score(self, user: str, response: str) -> float:
        """
        TF-IDFæ–°è¦æ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        
        Args:
            user: ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè©±
            response: AIå¿œç­”
        Returns:
            float: æ–°è¦æ€§ã‚¹ã‚³ã‚¢ï¼ˆ0.0-1.0ï¼‰
        """
    
    def annotate_text(self, text: str, context_window: int = 50) -> List[Dict]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã®è‡ªå‹•ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
        
        Returns:
            List[Dict]: ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å€™è£œãƒªã‚¹ãƒˆ
        """
```

#### A.9 æ—¢çŸ¥ã®åˆ¶ç´„ã¨æ³¨æ„äº‹é …

1. **ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡**: å¤§è¦æ¨¡è¾æ›¸ï¼ˆ>10ä¸‡èªï¼‰ä½¿ç”¨æ™‚ã¯2GBä»¥ä¸Šå¿…è¦
2. **å‡¦ç†æ™‚é–“**: 100ä¸‡æ–‡æ›¸ã®å‡¦ç†ã«ç´„2æ™‚é–“
3. **ã‚«ãƒ†ã‚´ãƒªæ•°**: å¿…é ˆ11ã‚«ãƒ†ã‚´ãƒª + æ‹¡å¼µã‚«ãƒ†ã‚´ãƒªã¯æœ€å¤§20ã¾ã§
4. **ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º**: å˜ä¸€è¾æ›¸ãƒ•ã‚¡ã‚¤ãƒ«ã¯10MBä»¥ä¸‹ã‚’æ¨å¥¨
5. **æ–‡å­—ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°**: UTF-8ã®ã¿å¯¾å¿œï¼ˆBOMç„¡ã—ï¼‰

### B. è©³ç´°ä»•æ§˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³

#### B.1 å¿…é ˆ11ã‚«ãƒ†ã‚´ãƒªå®šç¾©

| ã‚«ãƒ†ã‚´ãƒªå | å¤‰æ•°å | ç”¨é€” | ãƒ‘ã‚¿ãƒ¼ãƒ³ä¾‹ |
|-----------|--------|------|-----------|
| å®šå‹è¡¨ç¾ | template_phrases | æ…£ç”¨çš„å®šå‹å¥ã®æ¤œå‡º | ã”è³ªå•ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ |
| è¬™éœè¡¨ç¾ | humble_phrases | è‡ªå·±å‘ä¸‹è¡¨ç¾ã®æ¤œå‡º | ã¾ã ã¾ã ã€ä¸å®Œå…¨ãªãŒã‚‰ |
| å®Ÿç¸¾åè© | achievement_nouns | æˆæœé–¢é€£åè©ã®æ¤œå‡º | æˆæœã€å®Ÿç¸¾ã€é”æˆ |
| é”æˆå‹•è© | achievement_verbs | å®Ÿç¸¾å‹•è©ã®æ¤œå‡º | é”æˆã™ã‚‹ã€æˆåŠŸã™ã‚‹ |
| è©•ä¾¡å½¢å®¹è© | evaluative_adjectives | è‚¯å®šçš„è©•ä¾¡èªã®æ¤œå‡º | ç´ æ™´ã‚‰ã—ã„ã€å„ªç§€ãª |
| æ„Ÿæƒ…èª | positive_emotion_words | è‚¯å®šçš„æ„Ÿæƒ…è¡¨ç¾ã®æ¤œå‡º | å¬‰ã—ã„ã€æ„Ÿå‹• |
| å¼·èª¿å‰¯è© | intensifiers | å¼·èª¿è¡¨ç¾ã®æ¤œå‡º | ã¨ã¦ã‚‚ã€éå¸¸ã« |
| æ¯”è¼ƒèª | comparative_terms | æ¯”è¼ƒè¡¨ç¾ã®æ¤œå‡º | ã‚ˆã‚Šã€ã¨æ¯”ã¹ã¦ |
| é€†æ¥åŠ©è© | contrastive_conjunctions | é€†æ¥è¡¨ç¾ã®æ¤œå‡º | ãŒã€ã‘ã‚Œã© |
| æ¨é‡è¡¨ç¾ | modal_expressions | éæ–­å®šè¡¨ç¾ã®æ¤œå‡º | ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€ã§ã—ã‚‡ã† |
| è‡ªå·±å‚ç…§èª | self_reference_words | AIè‡ªå·±è¨€åŠã®æ¤œå‡º | ç§ã€å½“AI |

#### B.2 æŠ½å‡ºãƒ«ãƒ¼ãƒ«ä»•æ§˜

##### B.2.1 extraction_rules.yamlæ§‹é€ 

```yaml
# ã‚«ãƒ†ã‚´ãƒªæ‹¡å¼µå¯èƒ½æ€§è¨­å®š
categories:
  template_phrases:
    extendable: false  # 11æ—¢å®šã‚«ãƒ†ã‚´ãƒªã¯æ‹¡å¼µä¸å¯
    patterns:
      - regex: "æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³"
        min_frequency: 10
      - keywords: ["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2"]
        min_frequency: 5
  custom_category_1:
    extendable: true   # ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ†ã‚´ãƒªã¯æ‹¡å¼µå¯èƒ½
    # ...

# æŠ½å‡ºãƒ«ãƒ¼ãƒ«å®šç¾©
category_name:
  patterns:
    - regex: "æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³"
      min_frequency: 10
    - keywords: ["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰1", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰2"]
      min_frequency: 5
  pos_sequences:
    - ["å“è©1", "å“è©2", "å“è©3"]
  ngram_range: [æœ€å°, æœ€å¤§]
  min_frequency: 5
  pos_filter: ["åè©", "å‹•è©", "å½¢å®¹è©"]
  semantic_filters:
    embedding_threshold: 0.8
```

##### B.2.2 å“è©åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°

```python
def extract_pos_sequences(text: str, patterns: List[List[str]]) -> List[Tuple[str, str]]:
    """å“è©åˆ—ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ãæŠ½å‡º"""
    tagger = Tagger()
    tokens = []
    pos_tags = []
    
    for word in tagger(text):
        if word.surface:
            tokens.append(word.surface)
            features = word.pos.split(',')
            pos = features[0]
            if len(features) > 1 and features[1] != '*':
                pos += f'-{features[1]}'
            pos_tags.append(pos)
    
    matches = []
    for pattern in patterns:
        pattern_len = len(pattern)
        for i in range(len(pos_tags) - pattern_len + 1):
            if match_pos_pattern(pos_tags[i:i+pattern_len], pattern):
                surface = ''.join(tokens[i:i+pattern_len])
                pos_seq = '|'.join(pos_tags[i:i+pattern_len])
                matches.append((surface, pos_seq))
    
    return matches
```

#### B.3 å¼±æ•™å¸«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆä»•æ§˜

##### B.3.1 ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å½¢å¼

```json
{
  "id": "auto_0",
  "user": "ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè©±",
  "response": "AIå¿œç­”",
  "annotations": [
    {
      "text": "ç´ æ™´ã‚‰ã—ã„",
      "start": 10,
      "end": 15,
      "category": "evaluative_adjectives",
      "confidence": 0.95
    }
  ],
  "weak_labels": {
    "social": 0.8,
    "avoidant": 0.1,
    "mechanical": 0.05,
    "self": 0.05
  },
  "novelty_features": {
    "tfidf_novelty": 0.65
  },
  "source": "auto_annotation"
}
```

##### B.3.2 ä¿¡é ¼åº¦è¨ˆç®—

```python
def calculate_confidence(phrase: str, text: str, start: int, end: int) -> float:
    """ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ä¿¡é ¼åº¦ã®è¨ˆç®—"""
    # åŸºæœ¬ä¿¡é ¼åº¦ï¼šãƒ•ãƒ¬ãƒ¼ã‚ºé•·ã«åŸºã¥ã
    base_confidence = min(len(phrase) / 20, 1.0)
    
    # æ–‡å¢ƒç•Œã§ã®å‡ºç¾ã¯ä¿¡é ¼åº¦ã‚’ä¸‹ã’ã‚‹
    if start == 0 or end == len(text):
        base_confidence *= 0.8
    
    # å¥èª­ç‚¹ã«éš£æ¥ã—ã¦ã„ã‚‹å ´åˆã¯ä¿¡é ¼åº¦ã‚’ä¸Šã’ã‚‹
    if (start > 0 and text[start-1] in 'ã€ã€‚') or \
       (end < len(text) and text[end] in 'ã€ã€‚'):
        base_confidence *= 1.1
    
    return min(base_confidence, 1.0)
```

#### B.4 ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ä»•æ§˜

##### B.4.1 changelog.jsonæ§‹é€ 

```json
{
  "versions": [
    {
      "timestamp": "20250705_120000",
      "file_hash": "sha256:a1b2c3d4e5f6...",
      "metadata": {
        "action": "merge",
        "source": "corpus_extraction",
        "extendable_categories": ["custom_category_1"]
      },
      "statistics": {
        "template_phrases": {
          "extendable": false,
          "added": ["æ–°è¦ãƒ•ãƒ¬ãƒ¼ã‚º1", "æ–°è¦ãƒ•ãƒ¬ãƒ¼ã‚º2"],
          "removed": ["å‰Šé™¤ãƒ•ãƒ¬ãƒ¼ã‚º1"],
          "total_before": 100,
          "total_after": 101,
          "change_rate": 0.03
        }
      },
      "coverage_metrics": {
        "total_phrases": 1500,
        "category_distribution": {
          "template_phrases": 101,
          "humble_phrases": 50
        },
        "avg_phrase_length": 12.5
      }
    }
  ]
}
```

##### B.4.2 å·®åˆ†è¨ˆç®—ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

```python
def calculate_diff(prev_data: Dict, curr_data: Dict) -> Dict:
    """ãƒãƒ¼ã‚¸ãƒ§ãƒ³é–“å·®åˆ†ã®è¨ˆç®—"""
    diff_stats = {}
    all_categories = set(list(prev_data.keys()) + list(curr_data.keys()))
    
    for category in all_categories:
        prev_items = set(prev_data.get(category, []))
        curr_items = set(curr_data.get(category, []))
        
        added = list(curr_items - prev_items)
        removed = list(prev_items - curr_items)
        
        diff_stats[category] = {
            "added": sorted(added),
            "removed": sorted(removed),
            "total_before": len(prev_items),
            "total_after": len(curr_items),
            "change_rate": (len(added) + len(removed)) / max(len(prev_items), 1)
        }
    
    return diff_stats
```

#### B.5 ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è©•ä¾¡ä»•æ§˜

##### B.5.1 Silhouette Scoreè¨ˆç®—

```python
from sklearn.metrics import silhouette_score
import numpy as np

class ClusteringEvaluator:
    def evaluate_clustering(self, embeddings: np.ndarray, labels: np.ndarray) -> Dict[str, Any]:
        """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°çµæœã®è©•ä¾¡"""
        score = silhouette_score(embeddings, labels)
        
        metrics = {
            "algorithm": "KMeans",
            "n_clusters": len(np.unique(labels)),
            "silhouette_score": float(score),
            "timestamp": datetime.utcnow().isoformat() + 'Z'
        }
        
        # ã‚¹ã‚³ã‚¢ãŒä½ã„å ´åˆã¯è­¦å‘Š
        if score < 0.25:
            metrics["warning"] = "Low silhouette score indicates poor clustering quality"
        
        return metrics
```

##### B.5.2 è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›

```python
def save_cluster_metrics(metrics: Dict[str, Any], output_dir: str) -> None:
    """ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è©•ä¾¡æŒ‡æ¨™ã®ä¿å­˜"""
    output_path = os.path.join(output_dir, "reports", "cluster_metrics.json")
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)
```

#### B.6 è¾æ›¸ã‚¨ãƒ³ãƒˆãƒªä»•æ§˜

##### B.6.1 canonical_keyæ­£è¦åŒ–ãƒ«ãƒ¼ãƒ«

```python
def generate_canonical_key(phrase: str) -> str:
    """æ­£è¦åŒ–ã‚­ãƒ¼ã®ç”Ÿæˆ"""
    # 1. Unicode NFKCæ­£è¦åŒ–
    normalized = unicodedata.normalize('NFKC', phrase)
    
    # 2. å°æ–‡å­—åŒ–ï¼ˆã²ã‚‰ãŒãªãƒ»ã‚«ã‚¿ã‚«ãƒŠã¯å¯¾è±¡å¤–ï¼‰
    normalized = normalized.lower()
    
    # 3. å…¨è§’â†’åŠè§’å¤‰æ›ï¼ˆè‹±æ•°å­—ã®ã¿ï¼‰
    import mojimoji
    normalized = mojimoji.zen_to_han(normalized, kana=False)
    
    return normalized
```

##### B.6.2 è¾æ›¸ã‚¨ãƒ³ãƒˆãƒªå½¢å¼

```yaml
template_phrases:
  - phrase: "ã”è³ªå•ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™"
    canonical_key: "ã”è³ªå•ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™"
  - phrase: "ã”è³ªå•ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚"
    canonical_key: "ã”è³ªå•ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€‚"

positive_emotion_words:
  - phrase: "ç´ æ™´ã‚‰ã—ã„"
    canonical_key: "ç´ æ™´ã‚‰ã—ã„"
  - phrase: "ã™ã°ã‚‰ã—ã„"  
    canonical_key: "ã™ã°ã‚‰ã—ã„"
```

#### B.7 TF-IDFæ–°è¦æ€§è¨ˆç®—ä»•æ§˜

##### B.7.1 novelty_top_kç®—å‡º

```python
def calculate_tfidf_novelty_with_threshold(user_text: str, response_text: str, 
                                         vectorizer_path: str, 
                                         top_k: float = 0.2) -> float:
    """
    TF-IDFæ–°è¦æ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—ï¼ˆä¸Šä½k%é–¾å€¤ä»˜ãï¼‰
    
    Args:
        user_text: ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè©±
        response_text: AIå¿œç­”
        vectorizer_path: TF-IDFãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ã®ãƒ‘ã‚¹
        top_k: ä¸Šä½ä½•%ã‚’æ–°è¦ã¨ã¿ãªã™ã‹ï¼ˆ0.2 = ä¸Šä½20%ï¼‰
    
    Returns:
        float: æ–°è¦æ€§ã‚¹ã‚³ã‚¢ï¼ˆ0.0-1.0ï¼‰
    """
    calc = TFIDFNoveltyCalculator()
    calc.load_model(vectorizer_path)
    
    # ãƒ™ãƒ¼ã‚¹ã®æ–°è¦æ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
    base_score = calc.compute(user_text, response_text)
    
    # ä¸Šä½k%åˆ¤å®š
    if base_score >= (1.0 - top_k):
        return 1.0  # é«˜æ–°è¦æ€§
    else:
        return base_score / (1.0 - top_k)  # ç·šå½¢ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
```

#### B.8 ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ä»•æ§˜

##### B.8.1 ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§

| ãƒ•ã‚¡ã‚¤ãƒ«å | å½¢å¼ | å†…å®¹ |
|-----------|------|------|
| `extraction_summary.json` | JSON | æŠ½å‡ºå€™è£œã®çµ±è¨ˆæƒ…å ± |
| `cluster_metrics.json` | JSON | ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°è©•ä¾¡æŒ‡æ¨™ |
| `coverage_report.json` | JSON | è¾æ›¸ã‚«ãƒãƒ¬ãƒƒã‚¸åˆ†æ |
| `annotation_stats.json` | JSON | ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³çµ±è¨ˆ |

##### B.8.2 JSONã‚¹ã‚­ãƒ¼ãƒå®šç¾©

```json
// cluster_metrics.json
{
  "algorithm": "string",
  "n_clusters": "integer",
  "silhouette_score": "number",
  "timestamp": "string (ISO8601)",
  "warning": "string (optional)"
}

// extraction_summary.json
{
  "timestamp": "string",
  "corpus_size": "integer",
  "categories": {
    "category_name": {
      "candidates": "integer",
      "min_frequency": "integer",
      "avg_length": "number"
    }
  }
}
```
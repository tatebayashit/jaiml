## ğŸ“Š vector_pretrainer v1.1 ã‚·ã‚¹ãƒ†ãƒ è¦æ±‚ä»•æ§˜æ›¸ - æ”¹è¨‚ç‰ˆ

### A. çµ±ä¸€è¨˜è¿°ã‚»ã‚¯ã‚·ãƒ§ãƒ³

#### A.1 æ¦‚è¦

**ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å**: vector_pretrainer v1.1

**ç›®çš„**: å¤–éƒ¨å¯¾è©±ã‚³ãƒ¼ãƒ‘ã‚¹ï¼ˆSNOW D18ãƒ»BCCWJç­‰ï¼‰ã‚’ç”¨ã„ã¦TF-IDFãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ã‚’äº‹å‰å­¦ç¿’ã—ã€JAIML v3.3ãŠã‚ˆã³ lexicon_expansion v2.0ã®`tfidf_novelty`ç‰¹å¾´é‡è¨ˆç®—ã«ä¾›çµ¦ã™ã‚‹ã€‚å†ç¾æ€§ãƒ»å®‰å…¨æ€§ãƒ»æ‹¡å¼µæ€§ã‚’ç¢ºä¿ã™ã‚‹ãŸã‚ã€å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…±é€šã®è¨­å®šä½“ç³»ã«æº–æ‹ ã™ã‚‹ã€‚

#### A.2 ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹æˆã¨è²¬å‹™

```
src/vector_pretrainer/
â”œâ”€â”€ corpus/
â”‚   â”œâ”€â”€ raw/                 # å¤–éƒ¨é…å¸ƒã‚³ãƒ¼ãƒ‘ã‚¹ï¼ˆtxt, xmlç­‰ï¼‰
â”‚   â””â”€â”€ jsonl/               # æ­£è¦åŒ–æ¸ˆã¿JSONLå½¢å¼
â”œâ”€â”€ config/
â”‚   â””â”€â”€ tfidf_config.yaml    # TF-IDFå°‚ç”¨è¨­å®š
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ models/              # ä¸€æ™‚å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
â”‚   â”‚   â”œâ”€â”€ tfidf_vectorizer.joblib  # â†’ model/vectorizers/ã¸æ‰‹å‹•ã‚³ãƒ”ãƒ¼
â”‚   â”‚   â””â”€â”€ metadata.json             # â†’ model/vectorizers/ã¸æ‰‹å‹•ã‚³ãƒ”ãƒ¼
â”‚   â””â”€â”€ logs/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ to_jsonl.py          # ã‚³ãƒ¼ãƒ‘ã‚¹å½¢å¼å¤‰æ›
â”‚   â””â”€â”€ train_tfidf.py       # TF-IDFå­¦ç¿’
â””â”€â”€ README.md
```

**è²¬å‹™**:
- å¤§è¦æ¨¡ã‚³ãƒ¼ãƒ‘ã‚¹ã®å‰å‡¦ç†ã¨JSONLå½¢å¼ã¸ã®çµ±ä¸€
- TF-IDFãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ã®å­¦ç¿’ã¨æ°¸ç¶šåŒ–
- ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã‚‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã¨æ•´åˆæ€§ä¿è¨¼

#### A.3 å…¥å‡ºåŠ›ä»•æ§˜

**å…¥åŠ›å½¢å¼ï¼ˆJSONLï¼‰**:
```json
{
  "user": "ç™ºè©±è€…IDï¼ˆåŒ¿ååŒ–æ¸ˆã¿ï¼‰",
  "response": "ç™ºè©±ãƒ†ã‚­ã‚¹ãƒˆï¼ˆæ­£è¦åŒ–æ¸ˆã¿ï¼‰",
  "metadata": {
    "source": "ã‚³ãƒ¼ãƒ‘ã‚¹å",
    "topic": "è©±é¡Œã‚«ãƒ†ã‚´ãƒªï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰",
    "timestamp": "ç™ºè©±æ™‚åˆ»ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰"
  }
}
```

**å‡ºåŠ›å½¢å¼**:
1. `tfidf_vectorizer.joblib`: scikit-learn TfidfVectorizerã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
2. `metadata.json`:
```json
{
  "model_version": "1.1",
  "sklearn_version": "1.7.0",
  "creation_date": "2025-07-05T12:00:00Z",
  "corpus_stats": {
    "total_documents": 100000,
    "vocabulary_size": 50000
  },
  "config_hash": "sha256:...",
  "vectorizer_hash": "sha256:..."
}
```

#### A.4 ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å®šç¾©

**å…±é€šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆconfig/global.yamlã‹ã‚‰ç¶™æ‰¿ï¼‰**:
- `tokenizer`: "fugashi"
- `min_df`: 1
- `max_df`: 0.95
- `ngram_range`: [1, 1]

**TF-IDFå°‚ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆconfig/tfidf_config.yamlï¼‰**:
- `token_normalization`: "NFKC"
- `sublinear_tf`: true
- `use_idf`: true
- `smooth_idf`: true
- `norm`: "l2"

#### A.5 é–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆ

```
config/
â”œâ”€â”€ global.yaml              # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…±é€šè¨­å®š
â””â”€â”€ tfidf_config.yaml        # æœ¬ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å°‚ç”¨è¨­å®š

model/vectorizers/           # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æœ€çµ‚é…ç½®å…ˆ
â”œâ”€â”€ tfidf_vectorizer.joblib
â””â”€â”€ metadata.json

corpus/                      # å…¥åŠ›ã‚³ãƒ¼ãƒ‘ã‚¹
â”œâ”€â”€ raw/
â”‚   â””â”€â”€ SNOW_D18.txt
â””â”€â”€ jsonl/
    â””â”€â”€ combined.jsonl
```

#### A.6 ä½¿ç”¨ä¾‹ã¨ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³

**ã‚³ãƒ¼ãƒ‘ã‚¹å½¢å¼å¤‰æ›**:
```bash
python scripts/to_jsonl.py \
  --input corpus/raw/SNOW_D18.txt \
  --output corpus/jsonl/snow_d18.jsonl \
  --format plaintext \
  --anonymize true
```

**TF-IDFå­¦ç¿’**:
```bash
python scripts/train_tfidf.py \
  --corpus corpus/jsonl/combined.jsonl \
  --config config/tfidf_config.yaml \
  --output model/vectorizers/  # æœ€çµ‚é…ç½®å…ˆã«ç›´æ¥å‡ºåŠ›
```

#### A.7 CIæ¤œè¨¼é …ç›®

1. **è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«æ•´åˆæ€§**: `tfidf_config.yaml`ã¨`global.yaml`ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¸€è‡´
2. **tokenizerã®çµ±ä¸€**: å¿…ãš"fugashi"ãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã“ã¨
3. **å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼**: `.joblib`ã¨`metadata.json`ã®ä¸¡æ–¹ãŒç”Ÿæˆã•ã‚Œã‚‹ã“ã¨
4. **ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±**: `sklearn_version`ãŒå®Ÿè¡Œç’°å¢ƒã¨ä¸€è‡´ã™ã‚‹ã“ã¨
5. **å†ç¾æ€§ãƒ†ã‚¹ãƒˆ**: åŒä¸€å…¥åŠ›ã‹ã‚‰åŒä¸€ãƒ™ã‚¯ãƒˆãƒ«ãŒç”Ÿæˆã•ã‚Œã‚‹ã“ã¨
6. **ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸**: 80%ä»¥ä¸Šï¼ˆCIå…±é€šé–¾å€¤ï¼‰

#### A.8 ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹å®šç¾©ï¼ˆå‹æ³¨é‡ˆä»˜ãï¼‰

```python
from typing import Dict, List, Optional
import joblib
from sklearn.feature_extraction.text import TfidfVectorizer

class TFIDFTrainer:
    def __init__(self, config_path: str):
        """
        Args:
            config_path: tfidf_config.yamlã®ãƒ‘ã‚¹
        """
        self.config = self._load_config(config_path)
        self.vectorizer = None
    
    def train(self, corpus_path: str) -> None:
        """
        ã‚³ãƒ¼ãƒ‘ã‚¹ã‹ã‚‰TF-IDFãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ã‚’å­¦ç¿’
        
        Args:
            corpus_path: JSONLå½¢å¼ã®ã‚³ãƒ¼ãƒ‘ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
    
    def save(self, output_dir: str) -> Dict[str, str]:
        """
        å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
        
        Returns:
            Dict[str, str]: ä¿å­˜ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
                - "model": joblibå½¢å¼ã®ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«
                - "metadata": JSONå½¢å¼ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«
        """

class CorpusConverter:
    def convert_to_jsonl(self, input_path: str, output_path: str, 
                        format: str = "plaintext") -> int:
        """
        å„ç¨®å½¢å¼ã®ã‚³ãƒ¼ãƒ‘ã‚¹ã‚’JSONLå½¢å¼ã«å¤‰æ›
        
        Args:
            input_path: å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            output_path: å‡ºåŠ›JSONLãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            format: å…¥åŠ›å½¢å¼ï¼ˆ"plaintext", "xml", "csv"ï¼‰
            
        Returns:
            int: å¤‰æ›ã•ã‚ŒãŸæ–‡æ›¸æ•°
        """
```

#### A.9 æ—¢çŸ¥ã®åˆ¶ç´„ã¨æ³¨æ„äº‹é …

1. **ãƒ¡ãƒ¢ãƒªåˆ¶ç´„**: å¤§è¦æ¨¡ã‚³ãƒ¼ãƒ‘ã‚¹ï¼ˆ>1GBï¼‰ã®å‡¦ç†æ™‚ã¯æœ€å¤§4GBã®ãƒ¡ãƒ¢ãƒªãŒå¿…è¦
2. **å‡¦ç†æ€§èƒ½**: æœ€ä½1MB/åˆ†ï¼ˆ50MB/æ™‚é–“ï¼‰ã®å‡¦ç†é€Ÿåº¦ã‚’ä¿è¨¼
3. **ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º**: å‡ºåŠ›ã•ã‚Œã‚‹`.joblib`ãƒ•ã‚¡ã‚¤ãƒ«ã¯æœ€å¤§200MB
4. **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£**: Pickleå½¢å¼ã¯å®Œå…¨ç¦æ­¢ã€joblibå½¢å¼ã‚’å¿…é ˆã¨ã™ã‚‹
5. **åŒ¿ååŒ–è¦ä»¶**: å›ºæœ‰åè©ã¯`<PERSON>`, `<LOCATION>`ç­‰ã«ç½®æ›å¿…é ˆ

### B. è©³ç´°ä»•æ§˜ã‚»ã‚¯ã‚·ãƒ§ãƒ³

#### B.1 ã‚³ãƒ¼ãƒ‘ã‚¹å‰å‡¦ç†ä»•æ§˜

##### B.1.1 ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–

```python
def normalize_text(text: str) -> str:
    """ãƒ†ã‚­ã‚¹ãƒˆã®æ­£è¦åŒ–å‡¦ç†"""
    # 1. Unicodeæ­£è¦åŒ–ï¼ˆNFKCï¼‰
    text = unicodedata.normalize('NFKC', text)
    
    # 2. å…¨è§’è‹±æ•°å­—â†’åŠè§’å¤‰æ›
    text = mojimoji.zen_to_han(text, kana=False)
    
    # 3. HTMLã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã®ãƒ‡ã‚³ãƒ¼ãƒ‰
    text = html.unescape(text)
    
    # 4. åˆ¶å¾¡æ–‡å­—ã®é™¤å»
    text = re.sub(r'[\x00-\x1f\x7f-\x9f]', '', text)
    
    # 5. é€£ç¶šç©ºç™½ã®æ­£è¦åŒ–
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text
```

##### B.1.2 åŒ¿ååŒ–å‡¦ç†

æœ¬ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®åŒ¿ååŒ–å‡¦ç†ã¯ã€ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼ä¿è­·ã®ç¢ºå®Ÿæ€§ã‚’é«˜ã‚ã‚‹ãŸã‚ã€è‡ªå‹•å‡¦ç†ã¨äººæ‰‹è£œå®Œã®2æ®µéšæ§‹æˆã¨ã™ã‚‹ã€‚

**è‡ªå‹•å‡¦ç†ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆNERãƒ™ãƒ¼ã‚¹ï¼‰**:
- spaCy + GiNZAï¼ˆæ¨™æº–ãƒ¢ãƒ‡ãƒ«: `ja_ginza_electra`ï¼‰ã«ã‚ˆã‚‹å›ºæœ‰è¡¨ç¾èªè­˜ã‚’å®Ÿæ–½
- æ¤œå‡ºã•ã‚ŒãŸã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’ä»¥ä¸‹ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã§ãƒã‚¹ã‚­ãƒ³ã‚°ï¼š
  - `PERSON` â†’ `<PERSON>`
  - `ORG` â†’ `<ORG>`
  - `LOC` â†’ `<LOCATION>`
  - `DATE` â†’ `<DATE>`
  - `TIME` â†’ `<TIME>`
  - `MONEY` â†’ `<MONEY>`
  - `PERCENT` â†’ `<PERCENT>`
- æ­£è¦è¡¨ç¾ã«ã‚ˆã‚‹è¿½åŠ ãƒã‚¹ã‚­ãƒ³ã‚°ï¼š
  - ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹: `[\w\.-]+@[\w\.-]+\.\w+` â†’ `<EMAIL>`
  - é›»è©±ç•ªå·: `\d{2,4}-\d{2,4}-\d{4}` â†’ `<PHONE>`
  - å„ç¨®IDç•ªå·: `[A-Z]{2,3}\d{6,10}` â†’ `<ID>`

**äººæ‰‹è£œå®Œãƒ•ã‚§ãƒ¼ã‚º**:
- è‡ªå‹•å‡¦ç†çµæœã‚’CSVå½¢å¼ã§å‡ºåŠ›ã—ã€Spreadsheetã¾ãŸã¯Webãƒ™ãƒ¼ã‚¹ã®å°‚ç”¨UIã§äººé–“ãŒãƒ¬ãƒ“ãƒ¥ãƒ¼
- ãƒã‚¹ã‚­ãƒ³ã‚°æ¼ã‚Œã‚„éå‰°ãƒã‚¹ã‚­ãƒ³ã‚°ã‚’ä¿®æ­£
- ä¿®æ­£å±¥æ­´ã¯ `.annotated_difflog.jsonl` ã«è¨˜éŒ²ï¼š
  ```json
  {
    "document_id": "doc_001",
    "position": {"start": 45, "end": 50},
    "original": "å±±ç”°å¤ªéƒ",
    "masked": "<PERSON>",
    "annotator": "reviewer_01",
    "timestamp": "2025-07-05T14:30:00Z"
  }
  ```

**è¨­å®šç®¡ç†**:
- GiNZAãƒ¢ãƒ‡ãƒ«åã¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯ `config/global.yaml` ã® `common.ginza_model` ã§æŒ‡å®š
- å…¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã§åŒä¸€è¨­å®šã‚’å‚ç…§ã—ã€ä¸€è²«æ€§ã‚’ä¿è¨¼

**å®Ÿè£…ä¾‹**:
```python
import spacy
from typing import List, Tuple

class GiNZAAnonymizer:
    def __init__(self, model_name: str = "ja_ginza_electra"):
        self.nlp = spacy.load(model_name)
        self.entity_mapping = {
            "PERSON": "<PERSON>",
            "ORG": "<ORG>",
            "LOC": "<LOCATION>",
            "DATE": "<DATE>",
            "TIME": "<TIME>",
            "MONEY": "<MONEY>",
            "PERCENT": "<PERCENT>"
        }
    
    def anonymize(self, text: str) -> Tuple[str, List[dict]]:
        """
        ãƒ†ã‚­ã‚¹ãƒˆã®åŒ¿ååŒ–ã¨å¤‰æ›´ãƒ­ã‚°ã®ç”Ÿæˆ
        
        Returns:
            Tuple[str, List[dict]]: (åŒ¿ååŒ–æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆ, å¤‰æ›´ãƒ­ã‚°)
        """
        doc = self.nlp(text)
        anonymized_text = text
        changes = []
        
        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’é€†é †ã§å‡¦ç†ï¼ˆä½ç½®ãšã‚Œé˜²æ­¢ï¼‰
        for ent in reversed(doc.ents):
            if ent.label_ in self.entity_mapping:
                placeholder = self.entity_mapping[ent.label_]
                anonymized_text = (
                    anonymized_text[:ent.start_char] + 
                    placeholder + 
                    anonymized_text[ent.end_char:]
                )
                changes.append({
                    "position": {"start": ent.start_char, "end": ent.end_char},
                    "original": ent.text,
                    "masked": placeholder,
                    "entity_type": ent.label_
                })
        
        # æ­£è¦è¡¨ç¾ã«ã‚ˆã‚‹è¿½åŠ ãƒã‚¹ã‚­ãƒ³ã‚°
        anonymized_text = self._apply_regex_rules(anonymized_text, changes)
        
        return anonymized_text, changes
```

#### B.2 TF-IDFå­¦ç¿’ä»•æ§˜

##### B.2.1 Tokenizerã®å®Ÿè£…

```python
from fugashi import Tagger

class FugashiTokenizer:
    def __init__(self):
        self.tagger = Tagger()
    
    def __call__(self, text: str) -> List[str]:
        """fugashiã«ã‚ˆã‚‹å½¢æ…‹ç´ è§£æ"""
        tokens = []
        for word in self.tagger(text):
            if word.surface:
                tokens.append(word.surface)
        return tokens
```

##### B.2.2 TF-IDFãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ã®è¨­å®š

```python
def create_vectorizer(config: Dict) -> TfidfVectorizer:
    """è¨­å®šã«åŸºã¥ãTF-IDFãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ãƒ¼ã®ç”Ÿæˆ"""
    return TfidfVectorizer(
        tokenizer=FugashiTokenizer(),
        min_df=config['min_df'],
        max_df=config['max_df'],
        ngram_range=tuple(config['ngram_range']),
        sublinear_tf=config.get('sublinear_tf', True),
        use_idf=config.get('use_idf', True),
        smooth_idf=config.get('smooth_idf', True),
        norm=config.get('norm', 'l2')
    )
```

##### B.2.3 ãƒ¢ãƒ‡ãƒ«ä¿å­˜å½¢å¼

```python
def save_model(vectorizer: TfidfVectorizer, output_dir: str) -> Dict[str, str]:
    """ãƒ¢ãƒ‡ãƒ«ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜"""
    # 1. ãƒ¢ãƒ‡ãƒ«æœ¬ä½“ã®ä¿å­˜ï¼ˆjoblibå½¢å¼ï¼‰
    model_path = os.path.join(output_dir, 'tfidf_vectorizer.joblib')
    joblib.dump(vectorizer, model_path, compress=3)
    
    # 2. ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆã¨ä¿å­˜
    metadata = {
        'model_version': '1.1',
        'sklearn_version': sklearn.__version__,
        'creation_date': datetime.utcnow().isoformat() + 'Z',
        'corpus_stats': {
            'total_documents': vectorizer.n_features_in_,
            'vocabulary_size': len(vectorizer.vocabulary_)
        },
        'config_hash': hashlib.sha256(
            json.dumps(config, sort_keys=True).encode()
        ).hexdigest(),
        'vectorizer_hash': hashlib.sha256(
            open(model_path, 'rb').read()
        ).hexdigest()
    }
    
    metadata_path = os.path.join(output_dir, 'metadata.json')
    with open(metadata_path, 'w') as f:
        json.dump(metadata, f, indent=2)
    
    return {
        'model': model_path,
        'metadata': metadata_path
    }
```

#### B.3 ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã¨å†ç¾æ€§

##### B.3.1 Pickleä½¿ç”¨ã®ç¦æ­¢

```python
def load_model_safe(model_path: str) -> TfidfVectorizer:
    """å®‰å…¨ãªãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼ˆPickleè­¦å‘Šä»˜ãï¼‰"""
    if model_path.endswith('.pkl') or model_path.endswith('.pickle'):
        raise ValueError(
            "Pickle format is prohibited due to security risks. "
            "Use joblib format instead."
        )
    
    # joblibã§ã®èª­ã¿è¾¼ã¿
    try:
        model = joblib.load(model_path)
    except Exception as e:
        raise RuntimeError(f"Failed to load model: {e}")
    
    # ãƒãƒ¼ã‚¸ãƒ§ãƒ³æ¤œè¨¼
    metadata_path = model_path.replace('.joblib', '_metadata.json')
    if os.path.exists(metadata_path):
        with open(metadata_path) as f:
            metadata = json.load(f)
        
        if metadata['sklearn_version'] != sklearn.__version__:
            warnings.warn(
                f"Model was trained with sklearn {metadata['sklearn_version']}, "
                f"but current version is {sklearn.__version__}. "
                "This may cause compatibility issues.",
                RuntimeWarning
            )
    
    return model
```

##### B.3.2 å†ç¾æ€§ã®ä¿è¨¼

```python
def ensure_reproducibility():
    """å†ç¾æ€§ã‚’ä¿è¨¼ã™ã‚‹ãŸã‚ã®ç’°å¢ƒè¨­å®š"""
    # 1. ä¹±æ•°ã‚·ãƒ¼ãƒ‰å›ºå®š
    random.seed(42)
    np.random.seed(42)
    
    # 2. ãƒãƒƒã‚·ãƒ¥ã‚·ãƒ¼ãƒ‰å›ºå®š
    os.environ['PYTHONHASHSEED'] = '0'
    
    # 3. ä¸¦åˆ—å‡¦ç†ã®ç„¡åŠ¹åŒ–ï¼ˆæ±ºå®šè«–çš„å‡¦ç†ã®ãŸã‚ï¼‰
    os.environ['OMP_NUM_THREADS'] = '1'
    os.environ['MKL_NUM_THREADS'] = '1'
```